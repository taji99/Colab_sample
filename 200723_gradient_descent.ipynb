{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "name": "200723-gradient_descent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taji99/python_basic/blob/master/200723_gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya9S28H3OrAX",
        "colab_type": "text"
      },
      "source": [
        "# 勾配降下法\n",
        "誤差を次々と前の層へ伝播させて、重みとバイアスを少しずつ更新して最適化するために、勾配降下法というアルゴリズムを使用します。  \n",
        "今回は、勾配降下法の概要と勾配の求め方の概要を解説します。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bw3GZtuOrAZ",
        "colab_type": "text"
      },
      "source": [
        "## ●勾配降下法の概要\n",
        "$y$が$x_1, x_2,\\cdots,x_k,\\cdots$の関数だとします。  \n",
        "あるパラメータ$x_k$の変化量に対する結果$y(x_1, x_2,\\cdots,x_k,\\cdots)$の変化量の割合、すなわち勾配$\\frac{\\partial y}{\\partial x_k}$を求めて、この勾配に基づいてパラメータを調整し、$y$を最適化するアルゴリズムを勾配法といいます。 \n",
        "\n",
        "$\\partial$は偏微分の記号ですが、偏微分については以前に解説しています。    \n",
        "勾配降下法（gradient descent）は勾配法の一種で、結果が$y$の最小値に向かって降下するようにパラメータ$x_k$を変化させます。  \n",
        "以下の図にその概念を図で示します。  \n",
        "<br>\n",
        "**（図: 勾配降下法）**\n",
        "<br>  \n",
        "バックプロパゲーションにおいては損失関数により求めた誤差の値を起点に、ニューラルネットワークを遡って重みとバイアスの修正を行なっていくのですが、この際に勾配降下法を用いて修正量を決定します。  \n",
        "勾配降下法では、誤差が小さくなるように、ニューラルネットワークの重みとバイアスを調整します。  \n",
        "バックプロパゲーションにおける勾配降下法のイメージを以下の図に示します。  \n",
        "<br>\n",
        "**（図: 勾配降下法におけるニューラルネットワーク）**  \n",
        "<br>\n",
        "こちらのグラフでは、横軸の$w_{ij}$がある重み、縦軸の$E$が誤差です。  \n",
        "重みの値に応じて誤差は変化しますが、実際はこのような曲線の形状を知ることはできないので、足元の曲線の傾き（勾配）に応じて少しずつ重みを変化させていきます。  \n",
        "ネットワークの全ての重みをこの曲線を降下するように変化させていけば、誤差を次第に小さくしていくことができます。  \n",
        "この際の各重みの変化量は、この曲線の傾き、すなわち勾配で決まります。  \n",
        "バイアスの場合も同様です。  \n",
        "従って、ニューラルネットワークのすべての重みとバイアスを更新するためにまず必要なことは、すべての重みとバイアスに対する、誤差の勾配を求めることになります。  \n",
        "\n",
        "なお、$w_{ij}$の変化に対する$E$の変化は、必ずしも上の図のようなシンプルな曲線であるとは限りません。  \n",
        "以下の図のように局所的な最小値に囚われて、全体の最小値にたどり着くことができない場合もあります。  \n",
        "<br>\n",
        "**（図: 局所的な最小値へのトラップ）**  \n",
        "<br>\n",
        "このような最小値を**局所最適解**といいます。  \n",
        "それに対して、真の最小値を**大域最適解**といいます。  \n",
        "局所最適解を避けるためには様々な調整が必要になるのですが、これについては後ほど解説していきます。\n",
        "\n",
        "勾配降下法による重みとバイアスの更新は、$w$を重み、$b$をバイアス、$E$を誤差として、偏微分を用いた次の式で表すことができます。\n",
        "\n",
        "（式 1）\n",
        "$$ w \\leftarrow w-\\eta\\frac{\\partial E}{\\partial w} $$\n",
        "（式 2）\n",
        "$$ b \\leftarrow b-\\eta\\frac{\\partial E}{\\partial b} $$\n",
        "\n",
        "（式 1）、（式 2）において、矢印は重みの更新を表します。  \n",
        "\n",
        "$\\eta$は**学習係数**と呼ばれる定数で、$\\frac{\\partial E}{\\partial w}$と$\\frac{\\partial E}{\\partial b}$が勾配になります。  \n",
        "学習係数は、学習の速度を決める定数です。  \n",
        "0.1や0.01などの小さな値が使われることが多いのですが、小さすぎると学習に時間がかかりすぎたり、上の図に示すような局所最適解に囚われてしまう、といった問題が発生します。  \n",
        "しかしながら、学習係数が大きすぎても、誤差が収束しにくくなるという問題が発生します。  \n",
        "そのため、効率よく大域最適解にたどり着くためには、学習係数を適切に設定する必要があります。 \n",
        " \n",
        "（式 1）、（式 2）の勾配、$\\frac{\\partial E}{\\partial w}$と$\\frac{\\partial E}{\\partial b}$ですが、\n",
        "求めるためには数学的なテクニックが必要です。  \n",
        "これについては、のちほど解説します。  \n",
        "全ての勾配を求めた後、式（式 1）、（式 2）に基づき全ての重みとバイアスを更新します。 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hocSG_NOrAZ",
        "colab_type": "text"
      },
      "source": [
        "## ●勾配の求め方の概要\n",
        "それでは、勾配の求め方を解説します。  \n",
        "勾配さえ求めれば、式（式 1）、（式 2）に基づき重みとバイアスを更新することができます。  \n",
        "まずは、勾配の求め方の概要を解説します。  \n",
        "今回は、以下の図に示す3層のニューラルネットワークを想定し、各層の勾配を求めます。  \n",
        "<br>\n",
        "**（図: 誤差の逆伝播）**  \n",
        "<br>\n",
        "こちらの図には入力層、中間層、出力層の3層があり、中間層と出力層に重みとバイアスがあります。  \n",
        "入力層は入力を受け取り次の層に渡すのみなので、重みとバイアスはありません。  \n",
        "\n",
        "出力層では、誤差から重みとバイアスの勾配を求めます。  \n",
        "また、出力層への入力の勾配も同様に求めます。  \n",
        "順伝播では層の出力を伝播させますが、逆伝播ではこの入力の勾配を伝播させます。  \n",
        "中間層では、これを受け取り、これをもとに重みとバイアスの勾配、さらに中間層への入力の勾配を求めます。  \n",
        "入力の勾配が、ネットワークを遡上していくことになります。  \n",
        "入力の勾配を伝播させる理由は、のちほど解説します。  \n",
        "\n",
        "また、層の数が4層以上に増えたとしても、入力の勾配を伝播させることで、出力層以外は全て同じ方法で勾配を求めることができます。  \n",
        "そのため、3層のニューラルネットワークにおいて勾配を求めることができれば、それ以上層の数が増えても対応することができます。  "
      ]
    }
  ]
}