{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "name": "200723-layers_math.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taji99/python_basic/blob/master/200723_layers_math.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "RkFDojt8M8Tu",
        "colab_type": "text"
      },
      "source": [
        "# 層間の計算"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLcPcYhmM8Tw",
        "colab_type": "text"
      },
      "source": [
        "## ●2層間の接続\n",
        "ここからは、2つの層の間の順伝播を数式化します。  \n",
        "2つの層間の計算ができれば、残りも同じようにして計算することができます。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "G40tka6LM8Tx",
        "colab_type": "text"
      },
      "source": [
        "基本的に、上の層の全てのニューロンは、それぞれ下の層の全てのニューロンと接続されています。  \n",
        "\n",
        "前回解説した通り、ニューロンへのそれぞれの入力には重みをかけます。  \n",
        "重みの数は入力の数と等しいので、上の層のニューロン数を$m$とすると、下の層のニューロンは1つあたり$m$個の重みを持つことになります。  \n",
        "下の層のニューロン数$n$とすると、下の層には合計$m \\times n$個の重みが存在することになります。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvhEkSd1M8Ty",
        "colab_type": "text"
      },
      "source": [
        "## ●２層間の順伝播\n",
        "例えば上の層の1番目のニューロンから、下の層の2番目のニューロンへの入力の重みは$w_{12}$と表します。  \n",
        "重みは、上の層の全てのニューロンと、下の層の全てのニューロンのそれぞれの組み合わせごとに設定する必要がありますが、ここで、行列が役に立ちます。  \n",
        "以下のような$m \\times n$の行列に、下の層の全ての重みを格納することができます。\n",
        "\n",
        "（式 1）\n",
        "$$ W =\n",
        "    \\left(\n",
        "    \\begin{array}{cccc}\n",
        "      w_{11} & w_{12} & \\ldots & w_{1n} \\\\\n",
        "      w_{21} & w_{22} & \\ldots & w_{2n} \\\\\n",
        "      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "      w_{m1} & w_{m2} & \\ldots & w_{mn} \\\\\n",
        "    \\end{array}\n",
        "  \\right)\n",
        "$$\n",
        "\n",
        "$W$は重みを表す行列です。  \n",
        "\n",
        "また、上の層の出力（=下の層の入力）はベクトルで表すことができます。  \n",
        "上の層には$m$個のニューロンがあるので、ベクトルの要素数は$m$になります。$i$を上の層の添字、$j$を下の層の添字とし、$\\vec{y_i}$を上の層の出力を表すベクトル、$\\vec{x_j}$を下の層への入力を表すベクトルとすると、次のような表記が可能です。\n",
        "\n",
        "（式 2）\n",
        " $$ \\vec{y_i} = \\vec{x_j} = (x_1, x_2, \\cdots, x_m) $$\n",
        "\n",
        "上の層の出力は下の層の入力に等しくなります。  \n",
        "バイアスについては以前に解説しましたが、これもベクトルで表記することが可能です。  \n",
        "バイアスの数は下の層のニューロンの数に等しく、下の層のニューロンの数は$n$個なので、バイアス$\\vec{b_j}$は次のように表すことができます。\n",
        "\n",
        " $$ \\vec{b_j} =  (b_1, b_2, \\cdots, b_n) $$ \n",
        "\n",
        " また、下の層の出力の数はニューロンの数$n$に等しいので、ベクトル$\\vec{y_j}$を用いて次のように表記することができます。\n",
        "\n",
        " $$ \\vec{y_j} =  (y_1, y_2, \\cdots, y_n) $$ \n",
        "\n",
        "入力と重みの積の総和を求める必要がありますが、助かることに行列積を用いて一度に求めることができます。  \n",
        "（式 1）の$W$は下の層の重みを表す行列で、（式 2）の$\\vec{x_j}$は下の層への入力なので、$\\vec{x_j}$を1 x mの行列と考えると、以下の行列積で、各ニューロンにおける入力と重みの積の総和を求めることができます。  \n",
        "\n",
        "$$  \\begin{aligned} \\\\\n",
        " \\vec{x_j}W & = (x_1, x_2, \\cdots, x_m)\n",
        "\\left(\n",
        "    \\begin{array}{cccc}\n",
        "      w_{11} & w_{12} & \\ldots & w_{1n} \\\\\n",
        "      w_{21} & w_{22} & \\ldots & w_{2n} \\\\\n",
        "      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "      w_{m1} & w_{m2} & \\ldots & w_{mn} \\\\\n",
        "    \\end{array}\n",
        "  \\right) \\\\\n",
        "  & = (\\sum\\limits_{k=1}^m x_kw_{k1}, \\sum\\limits_{k=1}^m x_kw_{k2}, \\ldots, \\sum\\limits_{k=1}^m x_kw_{kn})\n",
        "  \\end{aligned}\n",
        "  $$ \n",
        "\n",
        "少々込み入った表記になりますが、行列積については以前に解説していますので、必要に応じて復習しましょう。  \n",
        "行列積の結果は要素数$n$のベクトルです。  \n",
        "このベクトルの各要素は、下の層の各ニューロンにおける重みと入力の積の総和になっていますね。  \n",
        "これにバイアス$\\vec{b_j}$を加えたものを$\\vec{u_j}$としますが、$\\vec{u_j}$は次のように求めます。\n",
        "\n",
        "$$  \\begin{aligned} \\\\\n",
        "\\vec{u_j} & = \\vec{x_j} W + \\vec{b_j} \\\\\n",
        "    & = (x_1, x_2, \\cdots, x_m)\n",
        "   \\left(\n",
        "    \\begin{array}{cccc}\n",
        "      w_{11} & w_{12} & \\ldots & w_{1n} \\\\\n",
        "      w_{21} & w_{22} & \\ldots & w_{2n} \\\\\n",
        "      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "      w_{m1} & w_{m2} & \\ldots & w_{mn} \\\\\n",
        "    \\end{array}\n",
        "  \\right) \n",
        " + (b_1, b_2, \\cdots, b_n) \\\\\n",
        "  & = (\\sum\\limits_{k=1}^m x_kw_{k1}+b_1, \\sum\\limits_{k=1}^m x_kw_{k2}+b_2, \\ldots, \\sum\\limits_{k=1}^m x_kw_{kn}+b_n)\n",
        "  \\end{aligned}\n",
        "  $$ \n",
        "\n",
        "$\\vec{u_j}$の各要素は、重みと入力の積の総和にバイアスを足したものになっています。  \n",
        "なお、$\\vec{u_j}$はNumpyのdot関数を用いて以下のように計算することができます。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G-ry2NOM8Tz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 概念だけ理解すること\n",
        "u = np.dot(x, w) + b  # x: 入力のベクトル w: 重みの行列 b: バイアスのベクトル"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3-53TB0M8T2",
        "colab_type": "text"
      },
      "source": [
        "次に、活性化関数を使用します。  \n",
        "ベクトル$\\vec{u_j}$の各要素を活性化関数に入れて処理し、下の層の出力を表すベクトル$\\vec{y_j}$を得ます。  \n",
        "\n",
        "（式 3）\n",
        "$$ \\begin{aligned}\n",
        "  \\vec{y_j} & = (y_1, y_2, \\cdots, y_n) \\\\\n",
        "  & = f(\\vec{u_j}) \\\\\n",
        "  & = f(\\vec{x_j}W + \\vec{b_j}) \\\\\n",
        "  & = (f(\\sum\\limits_{k=1}^m x_kw_{k1}+b_1), f(\\sum\\limits_{k=1}^m x_kw_{k2}+b_2), \\ldots, f(\\sum\\limits_{k=1}^m x_kw_{kn}+b_n))\n",
        "\\end{aligned} $$\n",
        "\n",
        "$\\vec{y_j}$の要素数は、下の層のニューロン数と同じ$n$になります。  \n",
        "$\\vec{y_j}$は、以前に扱った単一ニューロンの際の式を層全体に拡張したものになっていますね。  \n",
        "さらに下の層がある場合、$\\vec{y_j}$はその層への入力となります。    \n",
        "ニューロンを層として扱うことで、2つの層間の情報の伝播を数式にまとめることができました。  \n",
        "層の数が3つ以上になっても、（式 3）を用いて層から層へ次々に情報を順伝播することができます。  \n",
        "\n",
        "ニューラルネットワークは、層の数が増えて規模が大きくなれば、より生物に近い柔軟な認識・判断能力を持つことが可能になります。  \n",
        "そのためには各ニューロンの重みとバイアスを自動で調整する仕組みが必要になりますが、これについてはのちほど解説します。"
      ]
    }
  ]
}