{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "200726-simple_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taji99/python_basic/blob/master/200726_simple_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idgjdWWHQMca",
        "colab_type": "text"
      },
      "source": [
        "# シンプルなLSTMの実装\n",
        "LSTM層を用いてシンプルなニューラルネットワークを構築し、時系列データを学習します。  \n",
        "今回は、通常のRNNとLSTMを比較します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lRMtEmeQMcc",
        "colab_type": "text"
      },
      "source": [
        "## 訓練用データの作成\n",
        "訓練用のデータを作成します。  \n",
        "サイン関数に乱数でノイズを加えたデータを作成し、過去の時系列データから未来の値を予測できるようにします。  \n",
        "RNNの場合と同様に、正解は入力の時系列を一つ後にずらしたものにします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb7IJz-AQMcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_data = np.linspace(-2*np.pi, 2*np.pi)  # -2πから2πまで\n",
        "sin_data = np.sin(x_data) + 0.1*np.random.randn(len(x_data))  # sin関数に乱数でノイズを加える\n",
        "\n",
        "plt.plot(x_data, sin_data)\n",
        "plt.show()\n",
        "\n",
        "n_rnn = 10  # 時系列の数\n",
        "n_sample = len(x_data)-n_rnn  # サンプル数\n",
        "x = np.zeros((n_sample, n_rnn))  # 入力\n",
        "t = np.zeros((n_sample, n_rnn))  # 正解\n",
        "for i in range(0, n_sample):\n",
        "    x[i] = sin_data[i:i+n_rnn]\n",
        "    t[i] = sin_data[i+1:i+n_rnn+1]  # 時系列を入力よりも一つ後にずらす\n",
        "\n",
        "x = x.reshape(n_sample, n_rnn, 1)  # サンプル数、時系列の数、入力層のニューロン数\n",
        "print(x.shape)\n",
        "t = t.reshape(n_sample, n_rnn, 1)  # 今回は入力と同じ形状\n",
        "print(t.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-u1WgYOQMcg",
        "colab_type": "text"
      },
      "source": [
        "## SimpleRNNとLSTMの比較\n",
        "Kerasを使って通常のRNN、およびLSTMを構築します。  \n",
        "Kerasにおいて、LSTM層はSimpleRNN層と同じ方法で追加することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7La6nnJQMcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, LSTM\n",
        "\n",
        "batch_size = 8  # バッチサイズ\n",
        "n_in = 1  # 入力層のニューロン数\n",
        "n_mid = 20  # 中間層のニューロン数\n",
        "n_out = 1  # 出力層のニューロン数\n",
        "\n",
        "# 比較のための通常のRNN\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(SimpleRNN(n_mid, input_shape=(n_rnn, n_in), return_sequences=True))\n",
        "model_rnn.add(Dense(n_out, activation=\"linear\"))\n",
        "model_rnn.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "print(model_rnn.summary())\n",
        "\n",
        "# LSTM\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(LSTM(n_mid, input_shape=(n_rnn, n_in), return_sequences=True))\n",
        "model_lstm.add(Dense(n_out, activation=\"linear\"))\n",
        "model_lstm.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "print(model_lstm.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kg51tv0QMci",
        "colab_type": "text"
      },
      "source": [
        "SimpleRNNよりも、LSTMの方がパラメータがずっと多いですね。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ3Bp8xUQMcj",
        "colab_type": "text"
      },
      "source": [
        "## 学習\n",
        "構築したモデルを使って、学習を行います。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNG81L_qQMck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "epochs = 1000\n",
        "\n",
        "# 通常のRNN\n",
        "start_time = time.time()\n",
        "history_rnn = model_rnn.fit(x, t, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "print(\"学習時間 --通常のRNN--:\", time.time() - start_time)\n",
        "\n",
        "# LSTM\n",
        "start_time = time.time()\n",
        "history_lstm = model_lstm.fit(x, t, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "print(\"学習時間 --LSTM--:\", time.time() - start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb0JHo_RQMcl",
        "colab_type": "text"
      },
      "source": [
        "エポック数が同じ場合、パラメータ数が多いためLSTMの方が学習にずっと時間がかかります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt4lolK-QMcm",
        "colab_type": "text"
      },
      "source": [
        "## 学習の推移\n",
        "誤差の推移を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frTHytg4QMcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_rnn = history_rnn.history['loss']\n",
        "loss_lstm = history_lstm.history['loss']\n",
        "\n",
        "plt.plot(np.arange(len(loss_rnn)), loss_rnn, label=\"RNN\")\n",
        "plt.plot(np.arange(len(loss_lstm)), loss_lstm, label=\"LSTM\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bNJMP2mQMco",
        "colab_type": "text"
      },
      "source": [
        "モデルが複雑なため、LSTMの方が誤差の収束にエポック数が必要です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO3cp_QtQMcp",
        "colab_type": "text"
      },
      "source": [
        "## 学習済みモデルの使用\n",
        "それぞれの学習済みモデルを使って、サイン関数の次の値を予測します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5JNFUPtQMcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_rnn = x[0].reshape(-1) \n",
        "predicted_lstm = x[0].reshape(-1) \n",
        "\n",
        "for i in range(0, n_sample):\n",
        "    y_rnn = model_rnn.predict(predicted_rnn[-n_rnn:].reshape(1, n_rnn, 1))\n",
        "    predicted_rnn = np.append(predicted_rnn, y_rnn[0][n_rnn-1][0])\n",
        "    y_lstm = model_lstm.predict(predicted_lstm[-n_rnn:].reshape(1, n_rnn, 1))\n",
        "    predicted_lstm = np.append(predicted_lstm, y_lstm[0][n_rnn-1][0])\n",
        "\n",
        "plt.plot(np.arange(len(sin_data)), sin_data, label=\"Training data\")\n",
        "plt.plot(np.arange(len(predicted_rnn)), predicted_rnn, label=\"Predicted_RNN\")\n",
        "plt.plot(np.arange(len(predicted_lstm)), predicted_lstm, label=\"Predicted_LSTM\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrplcOrmQMcr",
        "colab_type": "text"
      },
      "source": [
        "LSTMを使ったモデルが、サインカーブを学習できていることが分かります。  \n",
        "\n",
        "このように、LSTMはRNNと同様に時系列データの学習ができるのですが、パラメータ数が多くモデルが複雑なため学習に時間がかかります。  \n",
        "今回の例からはLSTMのメリットはあまり分かりませんが、文脈が大事な自然言語処理などで、LSTMはその真価を発揮します。"
      ]
    }
  ]
}