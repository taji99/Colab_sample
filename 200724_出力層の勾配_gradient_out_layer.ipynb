{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "name": "200724-出力層の勾配_gradient_out_layer.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taji99/python_basic/blob/master/200724_%E5%87%BA%E5%8A%9B%E5%B1%A4%E3%81%AE%E5%8B%BE%E9%85%8D_gradient_out_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usDn_2_Z-b3h",
        "colab_type": "text"
      },
      "source": [
        "# 出力層の勾配\n",
        "出力層における勾配を導出しましょう。  \n",
        "回帰と分類で、異なる活性化関数と損失関数を使います。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-waQbb41-b3j",
        "colab_type": "text"
      },
      "source": [
        "## ●添字とニューロン数\n",
        "それでは、数式を用いて出力層における各勾配を求めます。  \n",
        "以降では、次の表のように各層におけるニューロンの添字とニューロン数を設定します。\n",
        "\n",
        "||||\n",
        "|:-:|:-:|:--|\n",
        "| 層 | ニューロンの添字 | ニューロン数 |\n",
        "| 入力層 | $i$ | $l$ |\n",
        "| 中間層 | $j$ | $m$ |\n",
        "| 出力層 | $k$ | $n$ |\n",
        "||||"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px4NVXRz-b3k",
        "colab_type": "text"
      },
      "source": [
        "## ●出力層の勾配\n",
        "出力層における、重みとバイアスの勾配を求めます。  \n",
        "$w_{jk}$を出力層における重み、$b_k$をバイアス、$u_k$を重みと入力の積の総和にバイアスを加えた値とします。  \n",
        "重みには中間層の出力が関わるので、添字はjとkの2つが必要になります。  \n",
        "また、中間層のニューロンの出力を$y_j$、出力層のニューロンの出力を$y_k$とします。  \n",
        "\n",
        "誤差は$E$とします。  \n",
        "\n",
        "まずは重みの勾配、すなわち$\\frac{\\partial E}{\\partial w_{jk}}$を求めます。  \n",
        "以降は、重みの勾配を以下のように$\\partial w_{jk}$とシンプルに記述することにします。  \n",
        "\n",
        "$$\\partial w_{jk} = \\frac{\\partial E}{\\partial w_{jk}}$$\n",
        "\n",
        "重みの勾配は、以前に解説した連鎖律を用いて以下のように展開できます。\n",
        "\n",
        "（式 1）\n",
        "$$ \\partial w_{jk}=\\frac{\\partial E}{\\partial w_{jk}}=\\frac{\\partial E}{\\partial u_k}\\frac{\\partial u_k}{\\partial w_{jk}} $$\n",
        "\n",
        "ここで、右辺の$\\frac{\\partial u_k}{\\partial w_{jk}}$の部分は、$y_j$が中間層の出力（出力層への入力）、$b_k$がバイアスなので次のように表せます。\n",
        "\n",
        "（式 2）\n",
        "$$ \\begin{aligned} \\\\\n",
        "\\frac{\\partial u_k}{\\partial w_{jk}} & = \\frac{\\partial (\\sum\\limits_{q=1}^m y_q w_{qk} + b_k)}{\\partial w_{jk}} \\\\\n",
        "& = \\frac{\\partial}{\\partial w_{jk}}(y_1 w_{1k}+y_2w_{2k}+\\cdots +y_jw_{jk}+\\cdots + y_mw_{mk} + b_k) \\\\\n",
        "& = y_j \n",
        "\\end{aligned} $$\n",
        "\n",
        "添字の$q$は$\\Sigma$による総和のために便宜上使用しているだけなので、特に意味はありません。  \n",
        "偏微分なので、$w_{jk}$がかかっている項以外は全て0になります。\n",
        "\n",
        "（式 1）式の右辺の$\\frac{\\partial E}{\\partial u_k}$の部分は、出力層のニューロンの出力を$y_k$とすると連鎖律により次のようになります。\n",
        "\n",
        "$$ \\frac{\\partial E}{\\partial u_k} = \\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial u_k} $$ \n",
        "\n",
        "すなわち、誤差を出力層のニューロンの出力で偏微分したものと、その出力を$u_k$で偏微分したものの積になります。  \n",
        "前者は損失関数を偏微分することで求めることができて、後者は活性化関数を偏微分することで求めることができます。  \n",
        "\n",
        "ここで、次のように$\\delta_{k}$を設定しておきます。  \n",
        "\n",
        "（式 3）\n",
        "$$ \\delta_{k} = \\frac{\\partial E}{\\partial u_k} = \\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial u_k} $$ \n",
        "\n",
        "この$\\delta_{k}$は、バイアスの勾配を求める際にも使用します。  \n",
        "（式 2）と（式 3）により、（式 1）は次の形になります。\n",
        "\n",
        "$$ \\partial w_{jk}=y_j\\delta_{k} $$\n",
        "\n",
        "重みの勾配$\\frac{\\partial E}{\\partial w_{jk}}$を、$y_j$と$\\delta_{k}$の積として表すことができました。 \n",
        "\n",
        "バイアスの勾配も同様にして求めることができます。  \n",
        "バイアスの勾配は、$\\partial b_k$と表記します。  \n",
        "\n",
        "$$ \\partial b_k =\\frac{\\partial E}{\\partial b_k} $$\n",
        "\n",
        "連鎖律により以下の関係が成り立ちます。\n",
        "\n",
        "（式 4）\n",
        "$$ \\partial b_k =\\frac{\\partial E}{\\partial b_k}=\\frac{\\partial E}{\\partial u_k}\\frac{\\partial u_k}{\\partial b_k} $$\n",
        "\n",
        "このとき、右辺の$\\frac{\\partial u_k}{\\partial b_{k}}$の部分は次のようになります。  \n",
        "\n",
        "$$ \\begin{aligned} \\\\\n",
        "\\frac{\\partial u_k}{\\partial b_k} & = \\frac{\\partial (\\sum\\limits_{q=1}^my_qw_{qk}+b_k)}{\\partial b_k} \\\\\n",
        "& = \\frac{\\partial}{\\partial b_k}(y_1w_{1k}+y_2w_{2k}+\\cdots +y_jw_{jk}+\\cdots + y_mw_{mk} + b_k) \\\\\n",
        "& = 1 \n",
        "\\end{aligned} $$\n",
        "\n",
        "（式 4）における$\\frac{\\partial E}{\\partial u_k}$の部分は、重みの勾配の場合と変わらないので、同様に$\\delta_{k}$とすると、上記の結果を踏まえて（式 4）は次の形になります。\n",
        "\n",
        "$$ \\partial b_k=\\delta_{k} $$\n",
        "\n",
        "このように、バイアスの場合勾配は$\\delta_{k}$に等しくなります。 \n",
        "\n",
        "以上により、重みとバイアスの勾配を、それぞれ$\\delta_{k}$を用いたシンプルな式で表すことができました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV521hQI-b3l",
        "colab_type": "text"
      },
      "source": [
        "## ●出力層における入力の勾配"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7YitqaA-b3m",
        "colab_type": "text"
      },
      "source": [
        "出力層では、1つ上の中間層における演算のために、予め$\\frac{\\partial E}{\\partial y_j}$、すなわち中間層の出力の勾配（ = 出力層の入力の勾配）を計算しておきます。  \n",
        "$\\frac{\\partial E}{\\partial y_j}$は、以降$\\partial y_j$と略記します。\n",
        "\n",
        "$$ \\partial y_j =\\frac{\\partial E}{\\partial y_j} $$\n",
        "\n",
        "出力層で予め計算しておいた$\\partial y_j$は、中間層で重みとバイアスの勾配を求めるために使用します。\n",
        "\n",
        "$\\partial y_j$は、以前に解説した多変数の合成関数の連鎖律により、次のようにして求めます。  \n",
        "\n",
        "（式 5）\n",
        "$$ \\begin{aligned} \\\\\n",
        "\\partial y_j & = \\frac{\\partial E}{\\partial y_j} \\\\\n",
        "& = \\sum_{r=1}^n\\frac{\\partial E}{\\partial u_r}\\frac{\\partial u_r}{\\partial y_j} \\\\\n",
        "\\end{aligned} $$\n",
        "\n",
        "$\\frac{\\partial E}{\\partial u_r}\\frac{\\partial u_r}{\\partial y_j}$を出力層の全てのニューロンで足し合わせればいいことになります。  \n",
        "$u_r$は入力と重みにバイアスを足した値ですが、出力層のニューロンの数だけあります。  \n",
        "\n",
        "添字の$r$は$\\Sigma$による総和のために便宜上使用しているだけなので、特に意味はありません。    \n",
        "この式において、$\\frac{\\partial u_r}{\\partial y_j}$は次のように求めることができます。\n",
        "\n",
        "$$ \\begin{aligned} \\\\\n",
        "\\frac{\\partial u_\n",
        "r}{\\partial y_j} & = \\frac{\\partial (\\sum\\limits_{q=1}^m y_q w_{qr} + b_r)}{\\partial y_j} \\\\\n",
        "& = \\frac{\\partial}{\\partial y_j}(y_1w_{1r}+y_2w_{2r}+\\cdots +y_jw_{jr}+\\cdots + y_mw_{mr} + b_r) \\\\\n",
        "& = w_{jr} \n",
        "\\end{aligned} $$\n",
        "\n",
        "以上に加えて、$\\delta_r = \\frac{\\partial E}{\\partial u_r}$により、（式 5）は以下のようになります。\n",
        "\n",
        "$$ \\partial y_j =  \\sum_{r=1}^n \\delta_r w_{jr} $$\n",
        "\n",
        "$\\partial y_j$を$\\delta_r$と$w_{jr}$の積の総和としてシンプルな形にまとめることができました。 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpzDXPNw-b3m",
        "colab_type": "text"
      },
      "source": [
        "## ●活性化関数と損失関数の適用 （回帰）\n",
        "回帰の場合は、活性化関数に恒等関数、損失関数に二乗和誤差を使い$\\delta_{k}$を求めます。  \n",
        "そのためには、最初に$\\delta_{k}$を求めます。  \n",
        "今回は、$\\delta_{k}$を求めるために（式 3）を次の形で使用します。  \n",
        "\n",
        "（式 4）\n",
        "$$ \\delta_{k} =  \\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial u_k} $$\n",
        "\n",
        "この式において、まずは$\\frac{\\partial E}{\\partial y_k}$を求めます。  \n",
        "これは、損失関数である二乗和誤差を出力$y_k$で偏微分することにより求めることができます。  \n",
        "\n",
        "（式 5）\n",
        "$$ \\begin{aligned} \\\\\n",
        "\\frac{\\partial E}{\\partial y_k} & = \\frac{\\partial}{\\partial y_k}(\\frac{1}{2} \\sum_{k}(y_k-t_k)^2) \\\\\n",
        "& = \\frac{\\partial}{\\partial y_k}(\\frac{1}{2}(y_0-t_0)^2+\\frac{1}{2}(y_1-t_1)^2+\\cdots+\\frac{1}{2}(y_k-t_k)^2+\\cdots+\\frac{1}{2}(y_n-t_n)^2) \\\\\n",
        "& = y_k-t_k\n",
        "\\end{aligned} $$\n",
        "\n",
        "係数の$\\frac{1}{2}$が、2を打ち消すために活躍していますね。  \n",
        "次に、$\\frac{\\partial y_k}{\\partial u_k}$を求めます。  \n",
        "これは、出力層の活性化関数を偏微分することで求めることができます。  \n",
        "出力層の活性化関数は恒等関数なので、次のように求めることができます。\n",
        "\n",
        "$$ \\begin{aligned} \\\\\n",
        "\\frac{\\partial y_k}{\\partial u_k} & = \\frac{\\partial u_k}{\\partial u_k} = 1\n",
        "\\end{aligned} $$\n",
        "\n",
        "この式と（式 5）を用いて、（式 4）は次のようになります。\n",
        "\n",
        "$$ \\delta_{k} = y_k-t_k $$ \n",
        "\n",
        "$\\delta_{k}$を求めることができました。  \n",
        "これを使うと、重みの勾配、バイアスの勾配、入力の勾配は今まで求めた式により次のようになります。\n",
        "\n",
        "$$ \\delta_{k} = y_k-t_k $$\n",
        "$$ \\partial w_{jk}=y_j\\delta_{k}  $$\n",
        "$$ \\partial b_k=\\delta_{k} $$ \n",
        "$$\\partial y_j = \\sum_{r=1}^n \\delta_r w_{jr}$$ \n",
        "\n",
        "以上により、回帰の場合の各勾配を求めることができました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0-59O4--b3n",
        "colab_type": "text"
      },
      "source": [
        "## ●活性化関数と損失関数の適用 （分類）\n",
        "分類問題のケースで勾配を導出してみましょう。  \n",
        "活性化関数にはソフトマックス関数、損失関数には交差エントロピー誤差を使います。  \n",
        "\n",
        "まずは、出力層の重みの勾配を導出します。  \n",
        "最初に$\\delta_{k}$を求めましょう。今回は、（式 3）を次の形で使用します。  \n",
        "\n",
        "（式 6）\n",
        "$$ \\delta_{k} = \\frac{\\partial E}{\\partial u_k} $$\n",
        "\n",
        "また、損失関数である交差エントロピー誤差と、活性化関数であるソフトマックス関数を次の形で表します。  \n",
        "\n",
        "（式 7）\n",
        "$$ E = -\\sum_{k} t_k \\log (y_k) $$\n",
        "\n",
        "（式 8）\n",
        "$$ y_k = \\frac{\\exp(u_k)}{\\sum\\limits_{k}\\exp(u_k)} $$\n",
        "\n",
        "ここで、$\\sum\\limits_{k}$は出力層の全ニューロンでの総和を意味します。  \n",
        "（式 7）に（式 8）を代入すると次のようになります。  \n",
        "\n",
        "$$ \\begin{aligned} \n",
        "E & = -\\sum_{k}t_k \\log (\\frac{\\exp(u_k)}{\\sum\\limits_{k}\\exp(u_k)}) \\\\\n",
        "\\end{aligned} $$\n",
        "\n",
        "この式は、$\\log \\frac{p}{q}=\\log p - \\log q$の関係により、次のように変形できます。\n",
        "\n",
        "（式 9）\n",
        "$$ \\begin{aligned}\n",
        "E & = -\\sum_{k}\\Bigl( t_k \\log \\bigl( \\exp(u_k) \\bigr) - t_k \\log \\sum\\limits_{k}\\exp(u_k) \\Bigr) \\\\\n",
        "& = -\\sum_{k}\\Bigl( t_k \\log \\bigl( \\exp(u_k) \\bigr)\\Bigr) + \\sum_{k} \\bigl( t_k  \\log\\sum\\limits_{k}\\exp(u_k) \\bigr) \\\\\n",
        "& = -\\sum_{k}\\Bigl( t_k \\log \\bigl( \\exp(u_k) \\bigr)\\Bigr) + \\bigl( \\sum_{k} t_k \\bigr) \\bigl( \\log\\sum\\limits_{k}\\exp(u_k) \\bigr) \n",
        "\\end{aligned} $$\n",
        "\n",
        "ここで、$\\log (\\exp(x)) = x$ であり、分類問題ではたった1つの正解値が1であとは0なので、$\\sum\\limits_{k} t_k = 1$となります。  \n",
        "従って、式（式 9）は次のようになります。  \n",
        "\n",
        "$$ \\begin{aligned}\n",
        "E & = -\\sum_{k} t_k u_k + \\log\\sum\\limits_{k}\\exp(u_k)\n",
        "\\end{aligned} $$\n",
        "\n",
        "これを式（式 6）に代入することで、$\\delta_{k}$を求めることができます。  \n",
        "\n",
        "$$ \\begin{aligned}\n",
        "\\delta_{k} & = \\frac{\\partial E}{\\partial u_k} \\\\\n",
        "& = \\frac{\\partial}{\\partial u_k} \\bigl(  -\\sum_{k} t_k u_k + \\log\\sum\\limits_{k}\\exp(u_k) \\bigr) \\\\\n",
        "& = -t_k + \\frac{\\exp(u_k)}{\\sum\\limits_{k}\\exp(u_k)} \\\\\n",
        "& = -t_k + y_k \\\\\n",
        "& = y_k - t_k\n",
        "\\end{aligned}  $$\n",
        "\n",
        "何と、$\\delta_{k}$は回帰問題の場合と同じ形になりました。  \n",
        "交差エントロピー誤差やソフトマックス関数は式が複雑に見えますが、分類問題も回帰問題と同様にシンプルな式で扱うことができます。  \n",
        "\n",
        "この$\\delta_{k}$を使うと、出力層において計算すべき各勾配を次のようにまとめることができます。\n",
        "\n",
        "$$ \\delta_{k}= y_k - t_k$$\n",
        "$$ \\partial w_{jk}=y_j\\delta_{k} $$\n",
        "$$ \\partial b_k=\\delta_{k} $$ \n",
        "$$ \\partial y_j = \\sum_{r=1}^n \\delta_r w_{jr} $$ \n",
        "\n",
        "以上により、分類の場合における各勾配を求めることができました。"
      ]
    }
  ]
}